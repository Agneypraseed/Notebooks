{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10j5CD-K1HXW"
      },
      "source": [
        "### HuggingFace Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9idRVL-yz1SF"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UYGFxeANAcZ",
        "outputId": "2a760d60-3b3f-429d-e44b-b155b346a9b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU available: True\n",
            "GPU name: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"GPU available:\", torch.cuda.is_available())\n",
        "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")\n",
        "\n",
        "# Make sure the model and data are sent to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccTHrq-Oz1SH",
        "outputId": "75a98cb4-7e3e-4ef9-d210-dbc23913b36e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "28996"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Initialize the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# See how many tokens are in the vocabulary\n",
        "tokenizer.vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2ltIYtyz1SK",
        "outputId": "c6b06924-1053-4b4d-c231-961da1aa0ac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['My', 'heart', 'is', 'Gene', '##rative']\n",
            "[1422, 1762, 1110, 9066, 15306]\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the sentence\n",
        "tokens = tokenizer.tokenize(\"My heart is Generative\")\n",
        "\n",
        "# Print the tokens\n",
        "print(tokens)\n",
        "\n",
        "# Show the token ids assigned to each token\n",
        "print(tokenizer.convert_tokens_to_ids(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UlvmS6ifz1SL"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7RYfVYLqz1SL"
      },
      "outputs": [],
      "source": [
        "# Load a pre-trained sentiment analysis model\n",
        "model_name = \"textattack/bert-base-uncased-imdb\"\n",
        "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3CfJiXQz1SM",
        "outputId": "a6c254bb-0d31-4140-a540-fb5954e0de4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment: Positive (70.16%)\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the input sequence\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "inputs = tokenizer(\"I am Generative AI\", return_tensors=\"pt\")\n",
        "\n",
        "# Make prediction\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs).logits\n",
        "    probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "    predicted_class = torch.argmax(probabilities)\n",
        "\n",
        "# Display sentiment result\n",
        "if predicted_class == 1:\n",
        "    print(f\"Sentiment: Positive ({probabilities[0][1] * 100:.2f}%)\")\n",
        "else:\n",
        "    print(f\"Sentiment: Negative ({probabilities[0][0] * 100:.2f}%)\")\n",
        "# Sentiment: Positive (88.68%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnIcKQB-1FcC"
      },
      "source": [
        "### HuggingFace Datasets library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P_qWvUc0fp3",
        "outputId": "3f1c9ead-5228-4ce6-92fa-f9b9fbbb1a5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "%pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jExUTBf4z1SN"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from IPython.display import HTML, display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uDMED0fuz5CU"
      },
      "outputs": [],
      "source": [
        "# Load the IMDB dataset, which contains movie reviews\n",
        "# and sentiment labels (positive or negative)\n",
        "dataset = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vmaixr2Uz7-7"
      },
      "outputs": [],
      "source": [
        "# Fetch a revie from the training set\n",
        "review_number = 42\n",
        "sample_review = dataset[\"train\"][review_number]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MENT0JSq0iLM",
        "outputId": "86dcb8eb-b03a-4bf6-eba9-d16deb10a466"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'text': \"WARNING: This review contains SPOILERS. Do not read if you don't want some points revealed to you before you watch the film.<br /><br />With a cast like this, you wonder whether or not the actors and actresses knew exactly what they were getting into. Did they see the script and say, `Hey, Close Encounters of the Third Kind was such a hit that this one can't fail.' Unfortunately, it does. Did they even think to check on the director's credentials? I mean, would YOU do a movie with the director of a movie called `Satan's Cheerleaders?' Greydon Clark, who would later go on to direct the infamous `Final Justice,' made this. It makes you wonder how the people of Mystery Science Theater 3000 could hammer `Final Justice' and completely miss out on `The Return.'<br /><br />The film is set in a small town in New Mexico. A little boy and girl are in the street unsupervised one night when a powerful flashlight beam.er.a spaceship appears and hovers over them. In probably the worst special effect sequence of the film, the ship spews some kind of red ink on them. It looked like Clark had held a beaker of water in from of the camera lens and dipped his leaky pen in it, so right away you are treated with cheese. Anyhow, the ship leaves and the adults don't believe the children. Elsewhere, we see Vincent Schiavelli, whom I find to be a terrific actor (watch his scenes in `Ghost' for proof, as they are outstanding), who is playing a prospector, or as I called him, the Miner 1949er. He steps out of the cave he is in, and he and his dog are inked by the ship. Twenty-five years go by, and the girl has grown up to be Cybill Shepherd, who works with her father, Raymond Burr, in studying unusual weather phenomena. Or something like that. Shepherd spots some strange phenomena in satellite pictures over that little New Mexico town, and she travels there to research it. Once she gets there, the local ranchers harass her, and blame her for the recent slew of cattle mutilations that have been going on, and deputy Jan-Michael Vincent comes to her rescue. From this point on, the film really drags as the two quickly fall for each other, especially after Vincent wards off the locals and informs Shepherd that he was the little boy that saw the ship with her twenty-five years earlier. While this boring mess is happening, Vincent Schiavelli, with his killer dog at his side, is walking around killing the cattle and any people he runs into with an unusual item. You know those glowing plastic sticks stores sell for trick-or-treaters at Halloween, the kind that you shake to make them glow? Schiavelli uses what looks like one of those glow sticks to burn incisions in people. It's the second-worst effect in the movie. Every time Schiavelli is on screen with the glow stick, the scene's atmosphere suddenly turns dark, like the filmmakers thought the glow stick needed that enhancement. It ends up making the movie look even cheaper than it is.<br /><br />And what does all this lead up to? It's hard to tell when the final, confusing scene arrives. See, Burr and his team of scientists try to explain the satellite images that Shepherd found as some kind of `calling card,' but none of it makes sense. Why do Shepherd and Vincent age and Schiavelli does not? Schiavelli explains why he is killing cattle and people and why he wants Shepherd dead, but even that doesn't make much sense when you really think about it. I mean, why doesn't he kill Jan-Michael Vincent? After all, he had twenty-five years to do it. And the aliens won't need him if Shepherd is dead anyhow, so why try to kill her? Speaking of the aliens, it is never clear what they really wanted out of Shepherd and Vincent. What is their goal? Why do they wait so long to intervene? How could they be so sure Shepherd would come back? Not that the answer to any of these and other questions would have made `The Return' any more pleasant. You would still have bad lines, really bad acting, particularly by Shepherd, cheesy effects, and poor direction. Luckily, the stars escaped from this movie. Cybill Shepherd soon went on to star in `Moonlighting' with Bruce Willis. Jan-Michael Vincent went on to be featured in dozens of B-movies, often in over-the-top parts. Raymond Burr made a pile of Perry Mason television movies right up until his death. Vincent Schiavelli went on to be a great character actor in a huge number of films. Martin Landau, who played a kooky law enforcement officer, quickly made the terrific `Alone in the Dark' and the awful `The Being' before rolling into the films he has been famous for recently. You can bet none of these stars ever want their careers to return to `The Return.' Zantara's score: 2 out of 10.\",\n",
              " 'label': 0}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sample_review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "XQESoCoK0uK-",
        "outputId": "b50b8305-0998-47f3-87c1-d7dcfd9edce8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "WARNING: This review contains SPOILERS. Do not read if you don't want some points revealed to you before you watch the film.<br /><br />With a cast like this, you wonder whether or not the actors and actresses knew exactly what they were getting into. Did they see the script and say, `Hey, Close Encounters of the Third Kind was such a hit that this one can't fail.' Unfortunately, it does. Did they even think to check on the director's credentials..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(HTML(sample_review[\"text\"][:450] + \"...\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENztbP-F0DOq",
        "outputId": "76ece8ab-78bf-4b49-f73a-e8c94a08c73b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment: Negative\n"
          ]
        }
      ],
      "source": [
        "if sample_review[\"label\"] == 1:\n",
        "    print(\"Sentiment: Positive\")\n",
        "else:\n",
        "    print(\"Sentiment: Negative\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGO5Zv3g_Ytf"
      },
      "source": [
        "### Hugging Face trainers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UIKC1g-x0qBD"
      },
      "outputs": [],
      "source": [
        "from transformers import (DistilBertForSequenceClassification,\n",
        "    DistilBertTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEUmLBZV_jW4",
        "outputId": "b8321b43-d292-416b-82ba-e0be0a002ea8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model = DistilBertForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=2\n",
        ")\n",
        "model.to(device)\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OkklKE9h_obI"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(data):\n",
        "    return tokenizer(data[\"text\"], padding=\"max_length\", truncation=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "N5TtvCPs_rt7"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"imdb\")\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    per_device_train_batch_size=64,\n",
        "    output_dir=\"./results\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=3,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "arNhzVo0_2Go",
        "outputId": "83425e57-c1ed-43d0-ea1d-7edb0c0c4249"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241228_172342-7q2nk094</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/greensciverto/huggingface/runs/7q2nk094' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/greensciverto/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/greensciverto/huggingface' target=\"_blank\">https://wandb.ai/greensciverto/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/greensciverto/huggingface/runs/7q2nk094' target=\"_blank\">https://wandb.ai/greensciverto/huggingface/runs/7q2nk094</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1173' max='1173' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1173/1173 55:06, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.252000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.140100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1173, training_loss=0.1840189667918798, metrics={'train_runtime': 3340.7474, 'train_samples_per_second': 22.45, 'train_steps_per_second': 0.351, 'total_flos': 9935054899200000.0, 'train_loss': 0.1840189667918798, 'epoch': 3.0})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b95rW9jDN_G5"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "dl_pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
