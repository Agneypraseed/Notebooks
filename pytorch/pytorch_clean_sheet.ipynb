{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2809,  0.2855,  0.1491],\n",
      "        [ 1.6699, -0.0231,  0.1176]])\n"
     ]
    }
   ],
   "source": [
    "size = (2, 3)\n",
    "x = torch.randn(*size)  # Creates a tensor with normally distributed random values (mean=0, std=1)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones tensor: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Zeros tensor: tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x_ones = torch.ones(*size)   # Tensor filled with ones\n",
    "x_zeros = torch.zeros(*size) # Tensor filled with zeros\n",
    "print(\"Ones tensor:\", x_ones)\n",
    "print(\"Zeros tensor:\", x_zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "L = [[1, 2, 3], [4, 5, 6]]\n",
    "x = torch.tensor(L)  # Converts a nested list to a tensor\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x: tensor([[999,   2,   3],\n",
      "        [  4,   5,   6]])\n",
      "Cloned y: tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n"
     ]
    }
   ],
   "source": [
    "y = x.clone()  # Creates a clone of tensor x\n",
    "x[0, 0] = 999  # Modify x to show that y is unaffected\n",
    "print(\"Original x:\", x)\n",
    "print(\"Cloned y:\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8734,  0.0622, -1.0020],\n",
      "        [ 0.1589, -1.3428,  0.3411],\n",
      "        [ 0.7378,  3.3918, -0.3597]], requires_grad=True)\n",
      "Before no_grad block, requires_grad: True\n",
      "Inside no_grad block, requires_grad: False\n",
      "tensor([[-1.7468,  0.1244, -2.0041],\n",
      "        [ 0.3179, -2.6857,  0.6823],\n",
      "        [ 1.4756,  6.7835, -0.7195]])\n",
      "After no_grad block, requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 3, requires_grad=True)\n",
    "print(x)\n",
    "print(\"Before no_grad block, requires_grad:\", x.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    x = x * 2\n",
    "    print(\"Inside no_grad block, requires_grad:\", x.requires_grad)\n",
    "\n",
    "print(x)\n",
    "print(\"After no_grad block, requires_grad:\", x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients of x: tensor([2., 4., 6.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)  # Track computation history\n",
    "y = x ** 2  # y = [1, 4, 9]\n",
    "z = y.sum()  # z = 1 + 4 + 9 = 14\n",
    "\n",
    "# Compute gradients\n",
    "z.backward()  # Computes dz/dx\n",
    "print(\"Gradients of x:\", x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)  # A tensor with shape (2, 3, 4)\n",
    "print(x.size())           # Returns the shape of the tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 3)\n",
    "b = torch.randn(2, 3)\n",
    "c = torch.cat((a, b), dim=0)  # Concatenates along dimension 0 (rows)\n",
    "print(c.size())               # Output size: (4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.7281,  1.3693, -0.9329, -1.7440],\n",
      "         [ 1.9872, -0.1911,  0.1413, -0.6456],\n",
      "         [-1.2565,  0.2459,  0.5283, -0.6760]],\n",
      "\n",
      "        [[-1.0694, -0.0727, -1.2120, -1.0388],\n",
      "         [-0.8967,  0.0956,  0.2473,  2.7595],\n",
      "         [ 1.3144, -0.5794, -0.4290,  0.3744]]])\n",
      "tensor([[[-0.7281,  1.9872, -1.2565],\n",
      "         [ 1.3693, -0.1911,  0.2459],\n",
      "         [-0.9329,  0.1413,  0.5283],\n",
      "         [-1.7440, -0.6456, -0.6760]],\n",
      "\n",
      "        [[-1.0694, -0.8967,  1.3144],\n",
      "         [-0.0727,  0.0956, -0.5794],\n",
      "         [-1.2120,  0.2473, -0.4290],\n",
      "         [-1.0388,  2.7595,  0.3744]]])\n",
      "torch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 3, 4)\n",
    "print(x)\n",
    "y = x.transpose(1, 2)  # Swaps dimensions 1 and 2\n",
    "print(y)\n",
    "print(y.size())        # Output size: (2, 4, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0239, -0.4895, -0.9352,  0.2771],\n",
      "        [-0.2216,  1.3820,  0.8290,  1.5302],\n",
      "        [ 0.7351,  1.1295,  0.9572, -1.0014]])\n",
      "tensor([[[-0.0239, -0.4895, -0.9352,  0.2771]],\n",
      "\n",
      "        [[-0.2216,  1.3820,  0.8290,  1.5302]],\n",
      "\n",
      "        [[ 0.7351,  1.1295,  0.9572, -1.0014]]])\n",
      "torch.Size([3, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 4)\n",
    "print(x)\n",
    "y = x.unsqueeze(1)  # Adds a dimension at index 1\n",
    "print(y)\n",
    "print(y.size())     # Output size: (3, 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 1, 3, 1)\n",
    "y = x.squeeze(dim=1)  # Removes the dimension at index 1 if it is of size 1\n",
    "print(y.size())       # Output size: (2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 58,  64],\n",
      "        [139, 154]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]])   # Shape (2, 3)\n",
    "B = torch.tensor([[7, 8], [9, 10], [11, 12]])  # Shape (3, 2)\n",
    "ret = A.mm(B)  # Matrix multiplication, resulting shape is (2, 2)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 50, 122])\n"
     ]
    }
   ],
   "source": [
    "A = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Shape (2, 3)\n",
    "x = torch.tensor([7, 8, 9])              # Shape (3,)\n",
    "ret = A.mv(x)  # Matrix-vector multiplication, resulting shape is (2,)\n",
    "print(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])  # Shape (2, 3)\n",
    "x_t = x.t()  # Transpose of x, resulting shape is (3, 2)\n",
    "print(x_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA Available:\", cuda_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 3)  # Tensor on CPU\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()  # Moves the tensor to GPU\n",
    "    print(\"Tensor on GPU:\", x.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "args = argparse.Namespace(disable_cuda=False)\n",
    "# Use GPU if available and not disabled, else use CPU\n",
    "if not args.disable_cuda and torch.cuda.is_available():\n",
    "    args.device = torch.device('cuda')\n",
    "else:\n",
    "    args.device = torch.device('cpu')\n",
    "\n",
    "print(\"Using device:\", args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor is on device: cpu\n",
      "Output: tensor([[ 0.0555,  0.0866],\n",
      "        [-0.7701, -0.7741],\n",
      "        [-0.3680, -0.7874],\n",
      "        [ 0.6082, -0.1511]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create a tensor and move it to the appropriate device\n",
    "x = torch.randn(4, 4).to(device)\n",
    "print(\"Tensor is on device:\", x.device)\n",
    "\n",
    "# Define a simple model and move it to the appropriate device\n",
    "model = torch.nn.Linear(4, 2).to(device)\n",
    "output = model(x)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[-0.8256, -0.8796]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define a linear layer from 4 input units to 2 output units\n",
    "linear_layer = nn.Linear(4, 2)\n",
    "x = torch.randn(1, 4)  # Input tensor of shape (batch_size, input_features)\n",
    "output = linear_layer(x)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 Loss: 0.5\n",
      "MSE Loss: 0.4166666567325592\n",
      "Cross Entropy Loss: 0.4170299470424652\n",
      "NLL Loss: 0.4170299470424652\n",
      "Poisson NLL Loss: 1.0351793766021729\n",
      "KL Divergence Loss: 0.013359417207539082\n",
      "BCE Loss: 0.18388253450393677\n",
      "BCEWithLogits Loss: 0.329584002494812\n",
      "Margin Ranking Loss: 1.0333333015441895\n",
      "Hinge Embedding Loss: 1.1333333253860474\n",
      "MultiLabel Margin Loss: 0.7999999523162842\n",
      "Smooth L1 Loss: 0.2083333283662796\n",
      "Soft Margin Loss: 0.4405346214771271\n",
      "MultiLabel Soft Margin Loss: 0.5379570722579956\n",
      "Multi Margin Loss: 0.21666666865348816\n",
      "Triplet Margin Loss: 0.0\n"
     ]
    }
   ],
   "source": [
    "## commonly used loss functions\n",
    "\n",
    "# L1Loss \n",
    "# L1Loss computes the mean absolute error (L1 norm) between predictions and targets.\n",
    "criterion = nn.L1Loss()\n",
    "pred = torch.tensor([0.0, 0.5, 1.0])\n",
    "target = torch.tensor([0.0, 1.0, 0.0])\n",
    "loss = criterion(pred, target)\n",
    "print(\"L1 Loss:\", loss.item())\n",
    "\n",
    "\n",
    "# MSELoss \n",
    "# computes the mean squared error (L2 norm), commonly used for regression tasks\n",
    "criterion = nn.MSELoss()\n",
    "pred = torch.tensor([0.0, 0.5, 1.0])\n",
    "target = torch.tensor([0.0, 1.0, 0.0])\n",
    "loss = criterion(pred, target)\n",
    "print(\"MSE Loss:\", loss.item())\n",
    "\n",
    "\n",
    "# CrossEntropyLoss\n",
    "# Used for multi-class classification. It combines log_softmax and nll_loss in one function.\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "pred = torch.tensor([[2.0, 1.0, 0.1]])  # Raw logits\n",
    "target = torch.tensor([0])  # Class index\n",
    "loss = criterion(pred, target)\n",
    "print(\"Cross Entropy Loss:\", loss.item())\n",
    "\n",
    "\n",
    "# CTCLoss\n",
    "# CTC loss is used for sequence-to-sequence problems where the output sequence length is shorter than the input sequence length, like speech to text.\n",
    "\n",
    "\n",
    "# NLLLoss\n",
    "# expects log-probabilities as input. It’s often used with log_softmax.\n",
    "criterion = nn.NLLLoss()\n",
    "log_probs = torch.log_softmax(torch.tensor([[2.0, 1.0, 0.1]]), dim=1)\n",
    "target = torch.tensor([0])\n",
    "loss = criterion(log_probs, target)\n",
    "print(\"NLL Loss:\", loss.item())\n",
    "\n",
    "# PoissonNLLLoss\n",
    "# Poisson Negative Log-Likelihood loss is used when the target is a count, and we want to model the Poisson distribution.\n",
    "criterion = nn.PoissonNLLLoss(log_input=True)\n",
    "input = torch.tensor([0.8, 0.9, 1.2, 1.5]).log()  # Log of the predicted rate\n",
    "target = torch.tensor([1.0, 1.0, 1.0, 1.0])  # Target count\n",
    "loss = criterion(input, target)\n",
    "print(\"Poisson NLL Loss:\", loss.item())\n",
    "\n",
    "\n",
    "# KLDivLoss\n",
    "# Kullback-Leibler divergence loss is used for measuring the difference between two probability distributions.\n",
    "criterion = nn.KLDivLoss(reduction='batchmean')\n",
    "input = torch.log(torch.tensor([0.2, 0.5, 0.3]))  # Log of predicted probabilities\n",
    "target = torch.tensor([0.1, 0.6, 0.3])  # Target probabilities\n",
    "loss = criterion(input, target)\n",
    "print(\"KL Divergence Loss:\", loss.item())\n",
    "\n",
    "\n",
    "# BCELoss\n",
    "# Used for binary classification problems. The input should be probabilities between 0 and 1.\n",
    "criterion = nn.BCELoss()\n",
    "pred = torch.tensor([0.8, 0.2, 0.1])\n",
    "target = torch.tensor([1.0, 0.0, 0.0])\n",
    "loss = criterion(pred, target)\n",
    "print(\"BCE Loss:\", loss.item())\n",
    "\n",
    "# BCEWithLogitsLoss\n",
    "# A numerically stable version of BCELoss that applies a sigmoid function internally.\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "logits = torch.tensor([1.5, -1.0, -0.5])\n",
    "target = torch.tensor([1.0, 0.0, 0.0])\n",
    "loss = criterion(logits, target)\n",
    "print(\"BCEWithLogits Loss:\", loss.item())\n",
    "\n",
    "\n",
    "# MarginRankingLoss\n",
    "# This loss is used for comparing pairs of inputs and measuring the relative ranking.\n",
    "criterion = nn.MarginRankingLoss(margin=1.0)\n",
    "input1 = torch.tensor([0.2, 0.5, 1.0])\n",
    "input2 = torch.tensor([0.3, 0.4, 0.9])\n",
    "target = torch.tensor([1.0, -1.0, 1.0])  # 1 means input1 > input2, -1 means input1 < input2\n",
    "loss = criterion(input1, input2, target)\n",
    "print(\"Margin Ranking Loss:\", loss.item())\n",
    "\n",
    "\n",
    "# HingeEmbeddingLoss\n",
    "# This loss is used for metric learning, often in tasks such as similarity-based ranking.\n",
    "criterion = nn.HingeEmbeddingLoss(margin=1.0)\n",
    "input = torch.tensor([0.6, -0.8, 1.0])\n",
    "target = torch.tensor([1.0, -1.0, 1.0])  # 1.0 if similar, -1.0 if dissimilar\n",
    "loss = criterion(input, target)\n",
    "print(\"Hinge Embedding Loss:\", loss.item())\n",
    "\n",
    "\n",
    "# MultiLabelMarginLoss\n",
    "# Used for multi-class, multi-label classification where labels are unordered.\n",
    "criterion = nn.MultiLabelMarginLoss()\n",
    "input = torch.tensor([[0.5, 1.0, -0.2], [0.1, -0.5, 0.3]])\n",
    "target = torch.tensor([[0, 1, 0], [0, 0, 1]])  # Multi-label target\n",
    "loss = criterion(input, target)\n",
    "print(\"MultiLabel Margin Loss:\", loss.item())\n",
    "\n",
    "\n",
    "# SmoothL1Loss\n",
    "# A combination of L1 and L2 loss, less sensitive to outliers than MSELoss.\n",
    "criterion = nn.SmoothL1Loss()\n",
    "pred = torch.tensor([0.0, 0.5, 1.0])\n",
    "target = torch.tensor([0.0, 1.0, 0.0])\n",
    "loss = criterion(pred, target)\n",
    "print(\"Smooth L1 Loss:\", loss.item())\n",
    "\n",
    "\n",
    "# SoftMarginLoss\n",
    "# SoftMarginLoss is used for binary classification with logistic regression.\n",
    "criterion = nn.SoftMarginLoss()\n",
    "input = torch.tensor([0.6, -0.4, 0.8])\n",
    "target = torch.tensor([1.0, -1.0, 1.0])  # Target class labels (1 or -1)\n",
    "loss = criterion(input, target)\n",
    "print(\"Soft Margin Loss:\", loss.item())\n",
    "\n",
    "\n",
    "# MultiLabelSoftMarginLoss\n",
    "# Similar to SoftMarginLoss but for multi-label classification tasks.\n",
    "criterion = nn.MultiLabelSoftMarginLoss()\n",
    "input = torch.tensor([[0.6, -0.4], [0.2, 0.7]])\n",
    "target = torch.tensor([[1.0, 0.0], [0.0, 1.0]])  # Multi-label targets\n",
    "loss = criterion(input, target)\n",
    "print(\"MultiLabel Soft Margin Loss:\", loss.item())\n",
    "\n",
    "\n",
    "# CosineEmbeddingLoss\n",
    "# Used to measure the cosine similarity between two tensors.\n",
    "\n",
    "\n",
    "\n",
    "# MultiMarginLoss\n",
    "# Used for multi-class classification with a margin to control misclassification.\n",
    "criterion = nn.MultiMarginLoss()\n",
    "input = torch.tensor([[0.5, -0.8, 1.0], [0.2, 0.5, -0.4]])\n",
    "target = torch.tensor([2, 1])  # Class indices\n",
    "loss = criterion(input, target)\n",
    "print(\"Multi Margin Loss:\", loss.item())\n",
    "\n",
    "\n",
    "# TripletMarginLoss\n",
    "# used for metric learning. It encourages the distance between an anchor and a positive example to be smaller than the distance between the anchor and a negative example by a margin.\n",
    "criterion = nn.TripletMarginLoss(margin=1.0)\n",
    "anchor = torch.tensor([1.0, 2.0])\n",
    "positive = torch.tensor([1.1, 2.1])\n",
    "negative = torch.tensor([3.0, 4.0])\n",
    "loss = criterion(anchor, positive, negative)\n",
    "print(\"Triplet Margin Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReLU Output: tensor([0., 0., 1., 2.])\n",
      "ELU Output: tensor([-0.6321,  0.0000,  1.0000])\n",
      "Leaky ReLU Output: tensor([-0.0100,  0.0000,  1.0000,  2.0000])\n",
      "GELU Output: tensor([-0.1587,  0.0000,  0.8413])\n",
      "Sigmoid Output: tensor([0.2689, 0.5000, 0.7311])\n",
      "Tanh Output: tensor([-0.7616,  0.0000,  0.7616])\n"
     ]
    }
   ],
   "source": [
    "# common activation functions\n",
    "\n",
    "# ReLU\n",
    "#The ReLU (Rectified Linear Unit) activation function replaces all negative values in the input tensor with zero, while leaving positive values unchanged:\n",
    "# f(x) = max(0, x).\n",
    "relu = nn.ReLU()\n",
    "x = torch.tensor([-1.0, 0.0, 1.0, 2.0])\n",
    "output = relu(x)\n",
    "print(\"ReLU Output:\", output)\n",
    "\n",
    "# ELU (Exponential Linear Unit) introduces negative values for inputs less than zero, making it smoother than ReLU\n",
    "elu = nn.ELU(alpha=1.0)\n",
    "x = torch.tensor([-1.0, 0.0, 1.0])\n",
    "output = elu(x)\n",
    "print(\"ELU Output:\", output)\n",
    "\n",
    "# LeakyReLU allows a small, non-zero gradient when the input is negative:\n",
    "# f(x) = x if x > 0, negative_slope * x if x <= 0.\n",
    "leaky_relu = nn.LeakyReLU(negative_slope=0.01)\n",
    "x = torch.tensor([-1.0, 0.0, 1.0, 2.0])\n",
    "output = leaky_relu(x)\n",
    "print(\"Leaky ReLU Output:\", output)\n",
    "\n",
    "# GELU (Gaussian Error Linear Unit) is a smooth, differentiable version of ReLU, commonly used in transformer models.\n",
    "gelu = nn.GELU()\n",
    "x = torch.tensor([-1.0, 0.0, 1.0])\n",
    "output = gelu(x)\n",
    "print(\"GELU Output:\", output)\n",
    "\n",
    "# Sigmoid squashes the input to a range between 0 and 1, making it useful for binary classification.\n",
    "sigmoid = nn.Sigmoid()\n",
    "x = torch.tensor([-1.0, 0.0, 1.0])\n",
    "output = sigmoid(x)\n",
    "print(\"Sigmoid Output:\", output)\n",
    "\n",
    "# Tanh squashes the input to a range between -1 and 1, often used in hidden layers of neural networks.\n",
    "tanh = nn.Tanh()\n",
    "x = torch.tensor([-1.0, 0.0, 1.0])\n",
    "output = tanh(x)\n",
    "print(\"Tanh Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Create a model (simple example)\n",
    "model = torch.nn.Linear(10, 2)\n",
    "\n",
    "# Example with Adam optimizer\n",
    "opt = optim.Adam(model.parameters(), lr=0.001)  # Adam optimizer with learning rate 0.001\n",
    "opt.step()  # Update model weights\n",
    "opt.zero_grad()  # Clear gradients after each step\n",
    "\n",
    "# Example with SGD (Stochastic Gradient Descent) optimizer\n",
    "opt = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # SGD with momentum\n",
    "opt.step()  # Update model weights\n",
    "opt.zero_grad()  # Clear gradients\n",
    "\n",
    "# SGD: Stochastic Gradient Descent, can have momentum to help accelerate convergence.\n",
    "# Adam: A combination of momentum and adaptive learning rates. AdamW is a variant that includes weight decay.\n",
    "# Adagrad, Adadelta: Adagrad adapts the learning rate for each parameter. Adadelta is an improved version that reduces the need to set a learning rate manually.\n",
    "# RMSprop: RMSprop is an adaptive learning rate optimizer, often used for training RNNs.\n",
    "# NAdam: A combination of Adam and Nesterov momentum.\n",
    "# RAdam: A rectified version of Adam that fixes some of Adam's issues with adaptive learning rates.\n",
    "# LBFGS: Limited-memory Broyden–Fletcher–Goldfarb–Shanno, used for small datasets or when high precision is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Create a simple model\n",
    "model = torch.nn.Linear(10, 2)\n",
    "\n",
    "# Example optimizer (Adam)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# LambdaLR - Custom learning rate schedule based on a lambda function\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda epoch: 0.95 ** epoch)\n",
    "scheduler.step()  # Update the learning rate after optimizer step\n",
    "\n",
    "# LambdaLR: The learning rate is updated according to a lambda function. You can customize this function to create a specific learning rate schedule.\n",
    "# MultiplicativeLR: The learning rate is updated by multiplying it with a factor defined in lr_lambda.\n",
    "# StepLR: The learning rate is decayed by a fixed factor (gamma) after a certain number of epochs (step_size).\n",
    "# MultiStepLR: Similar to StepLR, but allows you to specify exact epoch milestones where the learning rate should decay.\n",
    "# ExponentialLR: The learning rate is updated by a factor of gamma at each epoch.\n",
    "# CosineAnnealingLR: The learning rate decreases following a cosine curve, reaching a minimum value (eta_min).\n",
    "# ReduceLROnPlateau: It reduces the learning rate when a specific metric (like validation loss) stops improving.\n",
    "# CyclicLR: The learning rate cycles between a base_lr and max_lr over a number of steps, which can help escape local minima.\n",
    "# OneCycleLR: This scheduler is a variant of CyclicLR, where the learning rate first increases to max_lr and then decreases back down, following a specific \"one cycle\" pattern.\n",
    "# CosineAnnealingWarmRestarts: It allows the learning rate to restart after a specified number of epochs, with each restart followed by a cosine annealing pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([ 0.2998,  0.1806, -0.0758]), tensor(0))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# Create some dummy data and labels\n",
    "data = torch.randn(100, 3)  # 100 samples, each with 3 features\n",
    "labels = torch.randint(0, 2, (100,))  # 100 labels (binary classification)\n",
    "\n",
    "# Create TensorDataset\n",
    "dataset = TensorDataset(data, labels)\n",
    "\n",
    "# Access a sample from the dataset\n",
    "sample = dataset[0]  # Returns a tuple (data[0], labels[0])\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([-1.2187,  0.8016,  0.8722]), tensor(1))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset, TensorDataset\n",
    "import torch\n",
    "\n",
    "# Create two simple datasets\n",
    "data1 = torch.randn(5, 3)  # 5 samples, 3 features\n",
    "labels1 = torch.randint(0, 2, (5,))\n",
    "dataset1 = TensorDataset(data1, labels1)\n",
    "\n",
    "data2 = torch.randn(3, 3)  # 3 samples, 3 features\n",
    "labels2 = torch.randint(0, 2, (3,))\n",
    "dataset2 = TensorDataset(data2, labels2)\n",
    "\n",
    "# Concatenate the datasets\n",
    "combined_dataset = ConcatDataset([dataset1, dataset2])\n",
    "\n",
    "# Access a sample from the concatenated dataset\n",
    "sample = combined_dataset[6]  # Returns the 7th sample (index 6) from the concatenated dataset\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1131, -1.7908,  0.4184],\n",
      "        [ 0.3694,  1.7906, -1.6865]]) tensor([0, 1])\n",
      "tensor([[-2.6673, -1.5842, -0.0741],\n",
      "        [ 0.5427, -0.3060, -1.6506]]) tensor([0, 0])\n",
      "tensor([[ 0.2763, -0.2456, -0.5315],\n",
      "        [ 0.5851, -0.6923,  0.2732]]) tensor([0, 1])\n",
      "tensor([[ 0.2269,  0.2125, -1.6735],\n",
      "        [ 0.6308,  0.7755,  0.0099]]) tensor([0, 1])\n",
      "tensor([[-0.5852, -1.4539,  0.5168],\n",
      "        [ 0.1615,  0.7091, -0.1030]]) tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "# Create some dummy data\n",
    "data = torch.randn(10, 3)  # 10 samples, 3 features\n",
    "labels = torch.randint(0, 2, (10,))  # 10 labels for binary classification\n",
    "dataset = TensorDataset(data, labels)\n",
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Iterating over the DataLoader\n",
    "for batch_data, batch_labels in dataloader:\n",
    "    print(batch_data, batch_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
